{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the diffusion model architecture (e.g., using TSDiff)\n",
    "class TSDiff(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_steps):\n",
    "        super(TSDiff, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.embedding = nn.Embedding(num_steps, hidden_dim)  # Time step embedding\n",
    "\n",
    "        self.rnn = nn.LSTM(input_dim + hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, input_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Embed time steps\n",
    "        t_emb = self.embedding(t)\n",
    "        t_emb = t_emb.unsqueeze(1).expand(-1, x.size(1), -1)  # Expand to match input shape\n",
    "\n",
    "        # Concatenate input and time step embeddings\n",
    "        x_t = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "        # Process through RNN\n",
    "        output, _ = self.rnn(x_t)\n",
    "\n",
    "        # Project to output dimension\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "# Define the forward and reverse diffusion processes\n",
    "def forward_diffusion(x, t, beta_schedule):\n",
    "    \"\"\"\n",
    "    Implements the forward diffusion process.\n",
    "\n",
    "    Args:\n",
    "        x: Input time series data (batch_size, seq_length, input_dim).\n",
    "        t: Time steps (batch_size,).\n",
    "        beta_schedule: A function that returns the noise level (beta) for a given time step.\n",
    "\n",
    "    Returns:\n",
    "        Noisy time series data.\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x)  # Generate Gaussian noise with the same shape as x\n",
    "    alpha = 1 - beta_schedule(t)\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)  # Compute the cumulative product of alpha values\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar)\n",
    "\n",
    "    return sqrt_alpha_bar * x + sqrt_one_minus_alpha_bar * noise  # Add noise to the input\n",
    "\n",
    "def reverse_diffusion(model, x, t, beta_schedule):\n",
    "    \"\"\"\n",
    "    Implements a single step of the reverse diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: The TSDiff model.\n",
    "        x: Noisy time series data (batch_size, seq_length, input_dim).\n",
    "        t: Time steps (batch_size,).\n",
    "        beta_schedule: A function that returns the noise level (beta) for a given time step.\n",
    "\n",
    "    Returns:\n",
    "        Denoised time series data.\n",
    "    \"\"\"\n",
    "    alpha = 1 - beta_schedule(t)\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    beta = beta_schedule(t)\n",
    "\n",
    "    # Predict the noise component\n",
    "    predicted_noise = model(x, t)\n",
    "\n",
    "    # Compute the mean and variance of the conditional distribution q(x_{t-1} | x_t)\n",
    "    mean = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "    variance = beta * (1 - alpha_bar) / (1 - alpha)\n",
    "\n",
    "    # Sample from the conditional distribution to get x_{t-1}\n",
    "    if t > 0:  # Add noise if not at the final step\n",
    "        noise = torch.randn_like(x)\n",
    "    else:\n",
    "        noise = 0  # No noise at the final step\n",
    "\n",
    "    return mean + torch.sqrt(variance) * noise  # Return the denoised sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSDiff(input_dim=1, hidden_dim=64, num_layers=2, num_steps=1000)\n",
    "x = torch.randn(32, 10, 1)  # Batch of 32 time series, each with 10 time steps and 1 feature\n",
    "t = torch.randint(0, 1000, (32,))  # Batch of time steps\n",
    "output = model(x, t) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 1  # Dimensionality of your time series data\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_steps = 1000\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Create model, optimizer, and loss function\n",
    "model = TSDiff(input_dim, hidden_dim, num_layers, num_steps)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):  # Assuming you have a DataLoader for your data\n",
    "        t = torch.randint(0, num_steps, (batch_size,)).long()  # Sample random time steps\n",
    "\n",
    "        # Forward diffusion\n",
    "        noisy_x = forward_diffusion(x, t, linear_beta_schedule)\n",
    "\n",
    "        # Reverse diffusion (single step)\n",
    "        predicted_noise = model(noisy_x, t)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = mse_loss(predicted_noise, noise)  # Noise is the ground truth noise added in forward diffusion\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion models are increasingly being used for time-series data prediction tasks. While traditionally used for generative tasks like image and audio synthesis, recent research has shown their potential in time-series analysis.\n",
    "\n",
    "**How do Diffusion Models Work?**\n",
    "1. **Forward Diffusion:** The original time-series data is gradually corrupted with noise over multiple steps, eventually resembling pure noise.\n",
    "2. **Reverse Diffusion:** A neural network is trained to reverse this process, starting from pure noise and progressively denoising to reconstruct the original time-series.\n",
    "3. **Prediction:** By conditioning the reverse diffusion process on past observations, the model can generate future values, effectively performing time-series forecasting.\n",
    "\n",
    "**Key Advantages of Diffusion Models for Time Series:**\n",
    "\n",
    "* **Probabilistic Forecasting:** They provide a full probability distribution of future values, quantifying uncertainty in predictions.\n",
    "* **Flexibility:** They can handle diverse time-series data, including those with complex patterns and non-linear relationships.\n",
    "* **Generative Capabilities:** Besides forecasting, they can generate synthetic time-series data, useful for data augmentation and scenario analysis.\n",
    "\n",
    "**Current Research and Development:**\n",
    "\n",
    "There's ongoing active research in this area, with new models and techniques being developed to improve the performance and applicability of diffusion models for time-series tasks. For instance, some studies focus on incorporating domain knowledge into the diffusion process, while others explore their use for imputation (filling in missing values) in time-series data.\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "You can find more information in the following resources:\n",
    "\n",
    "* **Survey Paper:** Diffusion Models for Time Series Applications: A Survey (arXiv:2305.00624v1)\n",
    "* **Research Paper:** Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting\n",
    "\n",
    "If you have any specific questions or want to delve deeper into a particular aspect, feel free to ask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there isn't a standardized \"one-size-fits-all\" code for applying diffusion models in microgrid load prediction due to the novelty of this approach, I can provide you with a conceptual outline and some relevant code snippets that can guide your implementation:\n",
    "\n",
    "**Conceptual Outline:**\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   * Gather historical microgrid load data, along with relevant features (e.g., weather data, time of day, etc.).\n",
    "   * Clean and normalize the data to ensure proper model training.\n",
    "   * Split the data into training, validation, and testing sets.\n",
    "\n",
    "2. **Diffusion Model Architecture:**\n",
    "   * Choose a diffusion model architecture suitable for time-series data (e.g., TSDiff, as proposed in the paper \"Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting\").\n",
    "   * Adapt the model to incorporate microgrid-specific features.\n",
    "\n",
    "3. **Model Training:**\n",
    "   * Train the diffusion model on the preprocessed training data, gradually adding noise to the data in the forward diffusion process and training the model to reverse the process in the reverse diffusion step.\n",
    "   * Use the validation set to monitor model performance and prevent overfitting.\n",
    "\n",
    "4. **Load Prediction:**\n",
    "   * Condition the trained diffusion model on the latest observed microgrid load data and any other relevant features.\n",
    "   * Generate future load predictions using the reverse diffusion process.\n",
    "   * Evaluate the model's performance on the testing set using appropriate metrics (e.g., mean absolute error, root mean squared error).\n",
    "\n",
    "**Relevant Code Snippets (PyTorch Example):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# ... (Import other necessary libraries)\n",
    "\n",
    "# Define the diffusion model architecture (e.g., using TSDiff)\n",
    "class DiffusionModel(nn.Module):\n",
    "    # ... (Implementation of the model architecture)\n",
    "\n",
    "# Define the forward and reverse diffusion processes\n",
    "def forward_diffusion(x, t):\n",
    "    # ... (Implementation of the forward diffusion process)\n",
    "\n",
    "def reverse_diffusion(x, t):\n",
    "    # ... (Implementation of the reverse diffusion process)\n",
    "\n",
    "# Train the diffusion model\n",
    "model = DiffusionModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# ... (Training loop for the diffusion model)\n",
    "\n",
    "# Generate load predictions\n",
    "x = torch.tensor(latest_load_data) # Load the latest observed load data\n",
    "t = torch.tensor(prediction_steps)  # Define the number of prediction steps\n",
    "predictions = reverse_diffusion(x, t) # Generate predictions using reverse diffusion\n",
    "```\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "* **Model Architecture:** Explore different diffusion model architectures (e.g., TSDiff, Score-Based Generative Models) to find the one that best suits your microgrid load data and prediction requirements.\n",
    "* **Feature Engineering:** Carefully select and engineer relevant features (e.g., weather data, time of day, historical load patterns) to improve the model's accuracy.\n",
    "* **Uncertainty Quantification:** Utilize the probabilistic nature of diffusion models to quantify uncertainty in your load predictions, which can be crucial for decision-making in microgrid operations.\n",
    "\n",
    "Remember that this is a simplified example, and the actual implementation might involve additional complexities depending on your specific use case and data characteristics. Please refer to the relevant research papers and available libraries for more detailed guidance on implementing diffusion models for time-series data prediction.\n",
    "\n",
    "Feel free to ask if you have any further questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Here's a more complete code example for defining a diffusion model architecture in PyTorch, specifically using the TSDiff architecture as a reference point:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TSDiff(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_steps):\n",
    "        super(TSDiff, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.embedding = nn.Embedding(num_steps, hidden_dim)  # Time step embedding\n",
    "\n",
    "        self.rnn = nn.LSTM(input_dim + hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, input_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Embed time steps\n",
    "        t_emb = self.embedding(t)\n",
    "        t_emb = t_emb.unsqueeze(1).expand(-1, x.size(1), -1)  # Expand to match input shape\n",
    "\n",
    "        # Concatenate input and time step embeddings\n",
    "        x_t = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "        # Process through RNN\n",
    "        output, _ = self.rnn(x_t)\n",
    "\n",
    "        # Project to output dimension\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "1. **Initialization:**\n",
    "   * Takes `input_dim` (dimensionality of your time series data), `hidden_dim` (size of the hidden state in the LSTM), `num_layers` (number of LSTM layers), and `num_steps` (number of diffusion steps) as arguments.\n",
    "   * Initializes an embedding layer to learn representations for different time steps.\n",
    "   * Initializes a multi-layer LSTM network to process the time series data along with time step embeddings.\n",
    "   * Initializes a linear layer to project the LSTM output to the original input dimension.\n",
    "\n",
    "2. **Forward Pass:**\n",
    "   * Embeds the given time steps `t` using the embedding layer.\n",
    "   * Expands the time step embeddings to match the shape of the input `x`.\n",
    "   * Concatenates the input `x` and time step embeddings `t_emb` along the last dimension.\n",
    "   * Passes the concatenated input through the LSTM network.\n",
    "   * Projects the LSTM output through the linear layer to get the final output.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Time Step Embeddings:** This allows the model to learn different noise levels associated with different time steps during diffusion.\n",
    "* **LSTM:** Captures temporal dependencies in the time series data.\n",
    "* **Linear Layer:** Projects the hidden state of the LSTM to the output dimension, allowing for prediction of the original time series values.\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "model = TSDiff(input_dim=1, hidden_dim=64, num_layers=2, num_steps=1000)\n",
    "x = torch.randn(32, 10, 1)  # Batch of 32 time series, each with 10 time steps and 1 feature\n",
    "t = torch.randint(0, 1000, (32,))  # Batch of time steps\n",
    "output = model(x, t) \n",
    "```\n",
    "\n",
    "**Customization:**\n",
    "\n",
    "* You can experiment with different RNN architectures (e.g., GRU) or even transformer-based architectures.\n",
    "* Consider incorporating additional features relevant to microgrid load prediction (e.g., weather data, time of day) as input to the model.\n",
    "* Explore more advanced diffusion model techniques like score-based generative modeling for improved performance.\n",
    "\n",
    "Feel free to ask if you have any more questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Here's the code for implementing the forward and reverse diffusion processes for the TSDiff model we discussed earlier, along with explanations:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def forward_diffusion(x, t, beta_schedule):\n",
    "    \"\"\"\n",
    "    Implements the forward diffusion process.\n",
    "\n",
    "    Args:\n",
    "        x: Input time series data (batch_size, seq_length, input_dim).\n",
    "        t: Time steps (batch_size,).\n",
    "        beta_schedule: A function that returns the noise level (beta) for a given time step.\n",
    "\n",
    "    Returns:\n",
    "        Noisy time series data.\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x)  # Generate Gaussian noise with the same shape as x\n",
    "    alpha = 1 - beta_schedule(t)\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)  # Compute the cumulative product of alpha values\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar)\n",
    "\n",
    "    return sqrt_alpha_bar * x + sqrt_one_minus_alpha_bar * noise  # Add noise to the input\n",
    "\n",
    "def reverse_diffusion(model, x, t, beta_schedule):\n",
    "    \"\"\"\n",
    "    Implements a single step of the reverse diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: The TSDiff model.\n",
    "        x: Noisy time series data (batch_size, seq_length, input_dim).\n",
    "        t: Time steps (batch_size,).\n",
    "        beta_schedule: A function that returns the noise level (beta) for a given time step.\n",
    "\n",
    "    Returns:\n",
    "        Denoised time series data.\n",
    "    \"\"\"\n",
    "    alpha = 1 - beta_schedule(t)\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    beta = beta_schedule(t)\n",
    "\n",
    "    # Predict the noise component\n",
    "    predicted_noise = model(x, t)\n",
    "\n",
    "    # Compute the mean and variance of the conditional distribution q(x_{t-1} | x_t)\n",
    "    mean = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "    variance = beta * (1 - alpha_bar) / (1 - alpha)\n",
    "\n",
    "    # Sample from the conditional distribution to get x_{t-1}\n",
    "    if t > 0:  # Add noise if not at the final step\n",
    "        noise = torch.randn_like(x)\n",
    "    else:\n",
    "        noise = 0  # No noise at the final step\n",
    "\n",
    "    return mean + torch.sqrt(variance) * noise  # Return the denoised sample\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Forward Diffusion (`forward_diffusion`):**\n",
    "    * Takes the clean time series data `x`, time steps `t`, and a `beta_schedule` (a function defining how noise increases over time).\n",
    "    * Generates Gaussian noise.\n",
    "    * Calculates `alpha` (signal level) and its cumulative product `alpha_bar`.\n",
    "    * Adds noise to the input data according to the diffusion process equation.\n",
    "\n",
    "* **Reverse Diffusion (`reverse_diffusion`):**\n",
    "    * Takes the noisy data `x`, time steps `t`, the `model`, and the `beta_schedule`.\n",
    "    * Uses the model to predict the noise component in the data.\n",
    "    * Calculates the mean and variance of the conditional distribution using the predicted noise.\n",
    "    * Samples from the conditional distribution to obtain a less noisy version of the data.\n",
    "    * Continues this process iteratively until the original data is recovered.\n",
    "\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```python\n",
    "# Define a beta schedule (e.g., linear or cosine)\n",
    "def linear_beta_schedule(t):\n",
    "    return torch.linspace(0.0001, 0.02, t.shape[0])\n",
    "\n",
    "# ... (Other parts of your code, including model definition and data loading)\n",
    "\n",
    "# Perform forward diffusion\n",
    "noisy_data = forward_diffusion(x, t, linear_beta_schedule)\n",
    "\n",
    "# Perform reverse diffusion (multiple steps)\n",
    "for t_step in reversed(range(num_steps)):\n",
    "    x = reverse_diffusion(model, x, t_step, linear_beta_schedule)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
